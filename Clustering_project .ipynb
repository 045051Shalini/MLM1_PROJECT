{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337e3b3f-0902-413d-b07a-9a873f653f7d",
   "metadata": {},
   "source": [
    "# Project Details\n",
    "\n",
    "**Submitted by:** Shalini Chauhan-045051  \n",
    "**Submitted to:** Prof. Amarnath Mitra  \n",
    "**Project Name:** Unsupervised Learning  \n",
    "**Dataset:** Telecom Churn (99999 X 226)\n",
    "\n",
    "## About Dataset\n",
    "\n",
    "The dataset is a churn dataset of a telecommunication company with various features including:\n",
    "\n",
    "- `mou`: Minutes of Usage\n",
    "- `arpu`: Average Revenue Per User\n",
    "- `t2t`: Talktime to Talktime\n",
    "- `t2m`: Talktime to Mobile\n",
    "- `t2f`: Talktime to Fixed Line\n",
    "- `t2c`: Talktime to Customer Care\n",
    "- `ic`: Incoming Calls\n",
    "- `og`: Outgoing Calls\n",
    "- `std`: Standard\n",
    "- `isd`: International Subscriber Dialing\n",
    "- `spl`: Special\n",
    "- `roam`: Roaming\n",
    "- `loc`: Local\n",
    "- `max`: Maximum\n",
    "- `av`: Average\n",
    "- `vol`: Volume\n",
    "- `fb`: Facebook\n",
    "- `aon`: Age on Network\n",
    "- `vbc`: Volume Based Charging\n",
    "\n",
    "\n",
    "# Objective:\r\n",
    "The objective of this analysis is to gain insights into customer churn behavior in a telecom company dataset using clustering techniques. Specifically, we aim to identify distinct churn behavior patterns, understand their revenue impact, and explore service utilization trends within different customer segment along with comparison of two types of clustering..\n",
    "## Missing Data Report\n",
    "\n",
    "- Total Percentage of missing data = 15.91%\n",
    "- Missing values range from 0.60% to 74.85%\n",
    "- Imputation method used due to lack of discernible pattern in missing data\n",
    "\n",
    "## K-Means Clustering Report\n",
    "\n",
    "- Optimal number of clusters determined to be 7 for the entire dataset\n",
    "- Optimal number of clusters determined to be 5 for a subset of 25000 rows (Silhouette Score: 0.4474979398971488)\n",
    "  \n",
    "\n",
    "## Hierarchical Clustering Report\n",
    "\n",
    "- Silhouette score highest for model with 3 clusters (0.2883)\n",
    "- Silhouette score for 4 clusters (0.2801) and 5 clusters (0.2778)\n",
    "\n",
    "## Comparison of Memory and Time\n",
    "\n",
    "**K-Means:**\n",
    "- Memory Usage: Peak memory of 2882.79 MiB\n",
    "- Time Taken: CPU time of 24.3 seconds, Wall time of 11.5 seconds\n",
    "\n",
    "**Hierarchical Agglomerative:**\n",
    "- Memory Usage: Peak memory of 5350.38 MiB\n",
    "- Time Taken: CPU time of 1 minute 33 seconds, Wall time of 1 minute 37 seconds\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- K-Means clustering is more memory and time efficient compared to hierarchical agglomerative clustering\n",
    "- Hierarchical agglomerative clustering had scalability issues and couldn't be applied to the entire dataset\n",
    "- K-Means clustering is preferred for this dataset and subset size\n",
    "\n",
    "\r\n",
    "\r\n",
    "## Analysis:\r\n",
    "\r\n",
    "### K-Means Clustering:\r\n",
    "- **Optimal Number of Clusters:**\r\n",
    "  - K-means clustering identified 7 distinct clusters for the entire customer churn dataset.\r\n",
    "  - A subset of 25000 rows revealed 5 optimal clusters based on Silhouette Score.\r\n",
    "- **Cluster Characteristics:**\r\n",
    "  - **Churn Behavior Patterns:**\r\n",
    "    - Clusters likely represent different churn behavior patterns among telecom customers.\r\n",
    "    - Understanding these patterns can aid in predicting and mitigating churn, thereby improving customer retention rates.\r\n",
    "  - **Revenue Impact:**\r\n",
    "    - Clusters may exhibit varying revenue contributions, with some segments generating higher revenue despite churn propensity.\r\n",
    "    - Analyzing revenue patterns within clusters can inform targeted retention strategies and pricing adjustments.\r\n",
    "  - **Service Utilization Insights:**\r\n",
    "    - Different clusters may demonstrate preferences for specific services or usage patterns that correlate with churn likelihood.\r\n",
    "    - Identifying service utilization trends within clusters can guide service enhancements and customer engagement efforts.\r\n",
    "\r\n",
    "### Hierarchical Clustering:\r\n",
    "- **Silhouette Scores:**\r\n",
    "  - The silhouette score was highest for the model with 3 clusters, suggesting well-defined clusters with clear separations between churn behavior segments.\r\n",
    "- **Interpretation:**\r\n",
    "  - **Distinct Churn Profiles:**\r\n",
    "    - Hierarchical clustering reveals compact and well-separated churn behavior profiles, facilitating targeted interventions and personalized customer interactions.\r\n",
    "  - **Optimal Cluster Number:**\r\n",
    "    - While the silhouette score peaks at 3 clusters, additional clusters (4 and 5) offer nuanced insights into churn dynamics and customer segmentation.\r\n",
    "\r\n",
    "# Findings:\r\n",
    "\r\n",
    "## Strategic Implications:\r\n",
    "- The identified clusters provide actionable insights into customer churn behavior and revenue implications.\r\n",
    "- Telecom companies can leverage these insights to develop targeted retention strategies, optimize service offerings, and enhance overall customer satisfaction.\r\n",
    "\r\n",
    "## Algorithm Selection:\r\n",
    "- K-means clustering emerges as the preferred algorithm for customer churn analysis due to its efficiency and scalability.\r\n",
    "- Its ability to handle large datasets efficiently makes it a valuable tool for churn prediction and customer segmentation tasks.\r\n",
    "\r\n",
    "This analysis underscores the importance of cluster recognition in understanding customer churn dynamics and guiding strategic decision-making in the telecom industry. By leveraging clustering techniques, telecom companies can gain valuable insights into churn behavior patterns, revenue drivers, and service preferences, ultimately driving customer retention and business growth.\r\n",
    "\r\n",
    "# Implications:\r\n",
    "\r\n",
    "- **Tailored Retention Strategies:** Understanding distinct churn behavior patterns allows telecom companies to tailor retention strategies based on the needs and preferences of different customer segments. By addressing the specific reasons for churn within each cluster, companies can implement targeted interventions to improve customer retention rates.\r\n",
    "\r\n",
    "- **Revenue Optimization:** Analyzing revenue patterns within clusters enables companies to identify high-value customer segments that contribute significantly to revenue despite churn propensity. By focusing on retaining these valuable customers through personalized offers and incentives, companies can optimize revenue generation and profitability.\r\n",
    "\r\n",
    "- **Service Enhancement Opportunities:** Service utilization insights derived from cluster analysis highlight opportunities for enhancing service offerings and customer experience. By identifying service preferences and usage patterns within each cluster, companies can prioritize investments in areas that align with customer needs, g is preferred for this dataset and subset size\n",
    "\n",
    "## References\n",
    "\n",
    "- Prof. Amarnath Mitra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b644ce-d313-45f7-9f85-1c48207f4f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f9cc82-8e26-42ad-b447-5ab85ec5bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder # For Encoding Categorical Data [Nominal | Ordinal]\n",
    "from sklearn.preprocessing import OneHotEncoder # For Creating Dummy Variables of Categorical Data [Nominal]\n",
    "from sklearn.impute import SimpleImputer, KNNImputer # For Imputation of Missing Data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler # For Rescaling Data\n",
    "from sklearn.model_selection import train_test_split # For Splitting Data into Training & Testing Sets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ccf994c-f67d-4655-b823-9f7612326ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the ZIP file on GitHub\n",
    "zip_url = 'https://github.com/045051Shalini/MLM1_PROJECT/raw/main/telecom_churn_data.zip'\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(zip_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the ZIP file\n",
    "    with zipfile.ZipFile(BytesIO(response.content), 'r') as zip_ref:\n",
    "        # Assuming there's only one CSV file in the ZIP archive\n",
    "        csv_file_name = zip_ref.namelist()[0]\n",
    "        zip_ref.extractall('temp_folder')  # Extract files to a temporary folder\n",
    "\n",
    "    # Load the CSV file using pandas\n",
    "    csv_file_path = f'temp_folder/{csv_file_name}'\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbba5b9-16d9-49e7-a09d-0220dc3efe1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99999, 226)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f90ca-d232-45e7-bea4-a86342fe04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8c2d1-b9f7-4f60-9998-ffe1bb509c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec2691-1a12-4206-a1af-4b998f401fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]  \n",
    "total_missing = missing_data.sum() \n",
    "percent_missing = (total_missing / total_cells) * 100\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0444f-a8e3-48c1-9b60-47fbdfb591f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, -40:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcb143-2917-49ba-808e-ae89b65f583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,10),dpi=80)\n",
    "null_values = df.isnull().sum()\n",
    "null_values = null_values.reset_index()  # Resetting index to make column names accessible\n",
    "null_values.columns = ['Column', 'Missing_Count']  # Renaming columns for clarity\n",
    "\n",
    " # Adjust figure size if needed\n",
    "sns.barplot(x='Column', y='Missing_Count', data=null_values, color='skyblue')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Missing Values Count')\n",
    "plt.title('Missing Values Count in Each Column')\n",
    "plt.tight_layout()  # Adjust layout to prevent label cutoff\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce70086-84e6-4f95-baf0-44aceb685572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b8e8d-41ff-44f3-9311-189d97427e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming null_values DataFrame contains the column names and their corresponding missing values counts\n",
    "filtered_null_values = null_values[null_values['Missing_Count'] < 2000]\n",
    "filtered_null_values\n",
    "max_missing_count = filtered_null_values['Missing_Count'].max()\n",
    "max_missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35375fe6-98fc-4d6a-a34c-ee53cdca242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_null_values = null_values[(null_values['Missing_Count'] > 2000) & (null_values['Missing_Count'] < 10000)]\n",
    "filtered_null_values\n",
    "max_missing_count = filtered_null_values['Missing_Count'].max()\n",
    "max_missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ccadf-bf77-4ed9-9112-97c93cbb7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,10),dpi=80)\n",
    "df.isnull().sum()\n",
    "100* df.isnull().sum() / len(df)\n",
    "def percent_missing(df):\n",
    "    percent_nan = 100* df.isnull().sum() / len(df)\n",
    "    percent_nan = percent_nan[percent_nan>0].sort_values()\n",
    "    return percent_nan\n",
    "percent_nan = percent_missing(df)\n",
    "sns.barplot(x=percent_nan.index,y=percent_nan)\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d6893-7ea8-4402-a6ac-c98d7ef228be",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e7593-cefe-4d11-905e-a31113d6febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert percent_nan into a DataFrame\n",
    "percent_nan_df = pd.DataFrame(percent_nan.items(), columns=['Column Name', 'Percentage Missing'])\n",
    "\n",
    "# Sort DataFrame by percentage of missing values\n",
    "percent_nan_df.sort_values(by='Percentage Missing', inplace=True)\n",
    "\n",
    "# Dictionary to store the names of columns in each category\n",
    "column_names_by_category = {}\n",
    "\n",
    "# Extract unique percentages\n",
    "unique_percentages = percent_nan_df['Percentage Missing'].unique()\n",
    "unique_percentages.sort()\n",
    "\n",
    "# Iterate over consecutive pairs of unique percentages\n",
    "for i in range(len(unique_percentages) - 1):\n",
    "    start_percentage = unique_percentages[i]\n",
    "    end_percentage = unique_percentages[i + 1]\n",
    "    \n",
    "    # Get the names of columns with missing values within the current range\n",
    "    column_names = percent_nan_df[(percent_nan_df['Percentage Missing'] > start_percentage) & (percent_nan_df['Percentage Missing'] <= end_percentage)]['Column Name'].tolist()\n",
    "    \n",
    "    # Store the names in the dictionary\n",
    "    column_names_by_category[(start_percentage, end_percentage)] = column_names\n",
    "\n",
    "# Print the names of columns in each category\n",
    "for (start_percentage, end_percentage), column_names in column_names_by_category.items():\n",
    "    print(\"Percentage range:\", start_percentage, \"-\", end_percentage)\n",
    "    print(\"Number of columns:\", len(column_names))\n",
    "    print(\"Column names:\", column_names)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9095b-fde8-4da6-9927-1e01368ae2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_nan_df = pd.DataFrame(percent_nan) \n",
    "percent_nan_df.reset_index(inplace=True)\n",
    "percent_nan_df.columns = ['Column Name', 'Percentage Missing']\n",
    "percent_nan_df['Percentage Missing'].unique()\n",
    "#unique_values_column_0 = percent_nan[0].unique()\n",
    "#print(unique_values_column_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9796f9-21a6-4ff1-864b-fa3ed63daece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c462aa7-dbb1-4faf-8ba7-78a28295f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_nan[(percent_nan >10) &(percent_nan < 80)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a1950-48a1-4e53-a668-4691631080a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "100/len(df) \n",
    "print ([100/len(df)],'1 observation is 0.001%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9a12b-8355-48b4-a1d7-5af3dda12c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.histplot(data=percent_nan_df,x='Percentage Missing')\n",
    "plt.ylim(0,60)\n",
    "plt.xlim(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770c37d-795a-42ee-a7cf-7d5993d0a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the number of missing values in each row\n",
    "missing_values_counts = df.isnull().sum(axis=1)\n",
    "\n",
    "# Get unique missing values counts\n",
    "unique_counts = missing_values_counts.unique()\n",
    "\n",
    "# Initialize variables to store the total number of rows and the dictionary to store column counts\n",
    "total_rows = len(df)\n",
    "total_matched_rows = 0\n",
    "column_counts = {}\n",
    "\n",
    "# Iterate over each unique missing values count\n",
    "for count in unique_counts:\n",
    "    # Skip rows with no missing values\n",
    "    if count == 0:\n",
    "        continue\n",
    "    \n",
    "    # Find rows with the same missing values count\n",
    "    matching_rows = df[missing_values_counts == count]\n",
    "    \n",
    "    # Count the number of matching rows\n",
    "    matching_rows_count = len(matching_rows)\n",
    "    \n",
    "    # If there are matching rows, increment the total_matched_rows count and update the column_counts dictionary\n",
    "    if matching_rows_count > 1:\n",
    "        total_matched_rows += matching_rows_count\n",
    "        if count in column_counts:\n",
    "            column_counts[count] += matching_rows_count\n",
    "        else:\n",
    "            column_counts[count] = matching_rows_count\n",
    "\n",
    "print(\"Total number of rows:\", total_rows)\n",
    "print(\"Total number of rows with matching missing columns in other rows:\", total_matched_rows)\n",
    "print(\"Column counts for matched rows:\")\n",
    "for num_columns, count in column_counts.items():\n",
    "    print(f\"Number of columns matched: {num_columns}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047493b-32d8-4dc8-875a-ef05267dcac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2b923-db55-4c91-8bed-7a4e52b89c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.copy=df.drop(['mobile_number', 'circle_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35b02c-5c02-4673-a1ed-4ec4baa3b36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6bec49-8a78-4d8b-8c0c-e29983bf3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8c016-0de2-4de0-bfba-4464836ff7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['loc_ic_t2f_mou_9'].isnull()) & (df['loc_og_t2f_mou_9'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0131a6-955f-4215-bf8e-a01434f4a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "df_cat= df[[\n",
    "    'last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8', 'last_date_of_month_9',\n",
    "    'night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'night_pck_user_9',\n",
    "    'fb_user_6', 'fb_user_7', 'fb_user_8', 'fb_user_9'\n",
    "]]  \n",
    "\n",
    "\n",
    "# Non-categorical variables\n",
    "df_noncat = df[[\n",
    "    'loc_og_t2o_mou', 'std_og_t2o_mou', 'loc_ic_t2o_mou', 'arpu_6', 'arpu_7', 'arpu_8', 'arpu_9',\n",
    "    'onnet_mou_6', 'onnet_mou_7', 'onnet_mou_8', 'onnet_mou_9', 'offnet_mou_6', 'offnet_mou_7',\n",
    "    'offnet_mou_8', 'offnet_mou_9', 'roam_ic_mou_6', 'roam_ic_mou_7', 'roam_ic_mou_8', 'roam_ic_mou_9',\n",
    "    'roam_og_mou_6', 'roam_og_mou_7', 'roam_og_mou_8', 'roam_og_mou_9', 'loc_og_t2t_mou_6',\n",
    "    'loc_og_t2t_mou_7', 'loc_og_t2t_mou_8', 'loc_og_t2t_mou_9', 'loc_og_t2m_mou_6', 'loc_og_t2m_mou_7',\n",
    "    'loc_og_t2m_mou_8', 'loc_og_t2m_mou_9', 'loc_og_t2f_mou_6', 'loc_og_t2f_mou_7', 'loc_og_t2f_mou_8',\n",
    "    'loc_og_t2f_mou_9', 'loc_og_t2c_mou_6', 'loc_og_t2c_mou_7', 'loc_og_t2c_mou_8', 'loc_og_t2c_mou_9',\n",
    "    'loc_og_mou_6', 'loc_og_mou_7', 'loc_og_mou_8', 'loc_og_mou_9', 'std_og_t2t_mou_6', 'std_og_t2t_mou_7',\n",
    "    'std_og_t2t_mou_8', 'std_og_t2t_mou_9', 'std_og_t2m_mou_6', 'std_og_t2m_mou_7', 'std_og_t2m_mou_8',\n",
    "    'std_og_t2m_mou_9', 'std_og_t2f_mou_6', 'std_og_t2f_mou_7', 'std_og_t2f_mou_8', 'std_og_t2f_mou_9',\n",
    "    'std_og_t2c_mou_6', 'std_og_t2c_mou_7', 'std_og_t2c_mou_8', 'std_og_t2c_mou_9', 'std_og_mou_6',\n",
    "    'std_og_mou_7', 'std_og_mou_8', 'std_og_mou_9', 'isd_og_mou_6', 'isd_og_mou_7', 'isd_og_mou_8',\n",
    "    'isd_og_mou_9', 'spl_og_mou_6', 'spl_og_mou_7', 'spl_og_mou_8', 'spl_og_mou_9', 'og_others_6',\n",
    "    'og_others_7', 'og_others_8', 'og_others_9', 'total_og_mou_6', 'total_og_mou_7', 'total_og_mou_8',\n",
    "    'total_og_mou_9', 'loc_ic_t2t_mou_6', 'loc_ic_t2t_mou_7', 'loc_ic_t2t_mou_8', 'loc_ic_t2t_mou_9',\n",
    "    'loc_ic_t2m_mou_6', 'loc_ic_t2m_mou_7', 'loc_ic_t2m_mou_8', 'loc_ic_t2m_mou_9', 'loc_ic_t2f_mou_6',\n",
    "    'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8', 'loc_ic_t2f_mou_9', 'loc_ic_mou_6', 'loc_ic_mou_7', 'loc_ic_mou_8',\n",
    "    'loc_ic_mou_9', 'std_ic_t2t_mou_6', 'std_ic_t2t_mou_7', 'std_ic_t2t_mou_8', 'std_ic_t2t_mou_9',\n",
    "    'std_ic_t2m_mou_6', 'std_ic_t2m_mou_7', 'std_ic_t2m_mou_8', 'std_ic_t2m_mou_9', 'std_ic_t2f_mou_6',\n",
    "    'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8', 'std_ic_t2f_mou_9', 'std_ic_t2o_mou_6', 'std_ic_t2o_mou_7',\n",
    "    'std_ic_t2o_mou_8', 'std_ic_t2o_mou_9', 'std_ic_mou_6', 'std_ic_mou_7', 'std_ic_mou_8', 'std_ic_mou_9',\n",
    "    'total_ic_mou_6', 'total_ic_mou_7', 'total_ic_mou_8', 'total_ic_mou_9', 'spl_ic_mou_6', 'spl_ic_mou_7',\n",
    "    'spl_ic_mou_8', 'spl_ic_mou_9', 'isd_ic_mou_6', 'isd_ic_mou_7'\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d1191-6bf2-48a5-b89e-357a68528187",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # Strategy = median [When Odd Number of Categories Exists]\n",
    "\n",
    "# Now you can proceed with the imputation\n",
    "si_cat_fit = si_cat.fit_transform(df_cat.values)\n",
    "df_cat_si = pd.DataFrame(si_cat_fit, columns=df_cat.columns)\n",
    "df_cat_si.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13806494-1b5b-4b98-b34e-a1f6fa06ce4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92010c6b-a2ad-4878-bca9-311c900699da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "df_noncat_imputed = pd.DataFrame(imputer.fit_transform(df_noncat), columns=df_noncat.columns)\n",
    "df_noncat_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c7fae-c335-42b3-88cd-27b7127e49af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79f37a-863f-4bc0-8741-4dc844e1c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the specified columns to categorical type\n",
    "df_cat_si[['last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8', 'last_date_of_month_9',\n",
    "           'night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'night_pck_user_9',\n",
    "           'fb_user_6', 'fb_user_7', 'fb_user_8', 'fb_user_9']] = df_cat_si[[\n",
    "    'last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8', 'last_date_of_month_9',\n",
    "    'night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'night_pck_user_9',\n",
    "    'fb_user_6', 'fb_user_7', 'fb_user_8', 'fb_user_9']].astype('category')\n",
    "\n",
    "# Encode the categorical columns using cat.codes\n",
    "df_cat_si['last_date_of_month_6_code'] = df_cat_si['last_date_of_month_6'].cat.codes\n",
    "df_cat_si['last_date_of_month_7_code'] = df_cat_si['last_date_of_month_7'].cat.codes\n",
    "df_cat_si['last_date_of_month_8_code'] = df_cat_si['last_date_of_month_8'].cat.codes\n",
    "df_cat_si['last_date_of_month_9_code'] = df_cat_si['last_date_of_month_9'].cat.codes\n",
    "df_cat_si['night_pck_user_6_code'] = df_cat_si['night_pck_user_6'].cat.codes\n",
    "df_cat_si['night_pck_user_7_code'] = df_cat_si['night_pck_user_7'].cat.codes\n",
    "df_cat_si['night_pck_user_8_code'] = df_cat_si['night_pck_user_8'].cat.codes\n",
    "df_cat_si['night_pck_user_9_code'] = df_cat_si['night_pck_user_9'].cat.codes\n",
    "df_cat_si['fb_user_6_code'] = df_cat_si['fb_user_6'].cat.codes\n",
    "df_cat_si['fb_user_7_code'] = df_cat_si['fb_user_7'].cat.codes\n",
    "df_cat_si['fb_user_8_code'] = df_cat_si['fb_user_8'].cat.codes\n",
    "df_cat_si['fb_user_9_code'] = df_cat_si['fb_user_9'].cat.codes\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_cat_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5bf3c-297d-4082-a6f7-b7ce0593c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_columns= df_cat_si.filter(regex='_code$')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_encoded_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bb9d4-0293-48f5-9575-bd322b67ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dummy_cat = pd.get_dummies(df_encoded_columns,\n",
    "                               columns=['last_date_of_month_6_code', 'last_date_of_month_7_code', 'last_date_of_month_8_code', 'last_date_of_month_9_code',\n",
    "                                        'night_pck_user_6_code', 'night_pck_user_7_code', 'night_pck_user_8_code', 'night_pck_user_9_code',\n",
    "                                        'fb_user_6_code', 'fb_user_7_code', 'fb_user_8_code', 'fb_user_9_code'])\n",
    "\n",
    "df_dummy_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1aa937-2e6b-41fc-b88f-efedf0b05f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Standardization\n",
    "ss = StandardScaler()\n",
    "ss_fit = ss.fit_transform(df_noncat_imputed[['loc_og_t2o_mou', 'std_og_t2o_mou', 'loc_ic_t2o_mou', 'arpu_6', 'arpu_7', 'arpu_8', 'arpu_9',\n",
    "    'onnet_mou_6', 'onnet_mou_7', 'onnet_mou_8', 'onnet_mou_9', 'offnet_mou_6', 'offnet_mou_7',\n",
    "    'offnet_mou_8', 'offnet_mou_9', 'roam_ic_mou_6', 'roam_ic_mou_7', 'roam_ic_mou_8', 'roam_ic_mou_9',\n",
    "    'roam_og_mou_6', 'roam_og_mou_7', 'roam_og_mou_8', 'roam_og_mou_9', 'loc_og_t2t_mou_6',\n",
    "    'loc_og_t2t_mou_7', 'loc_og_t2t_mou_8', 'loc_og_t2t_mou_9', 'loc_og_t2m_mou_6', 'loc_og_t2m_mou_7',\n",
    "    'loc_og_t2m_mou_8', 'loc_og_t2m_mou_9', 'loc_og_t2f_mou_6', 'loc_og_t2f_mou_7', 'loc_og_t2f_mou_8',\n",
    "    'loc_og_t2f_mou_9', 'loc_og_t2c_mou_6', 'loc_og_t2c_mou_7', 'loc_og_t2c_mou_8', 'loc_og_t2c_mou_9',\n",
    "    'loc_og_mou_6', 'loc_og_mou_7', 'loc_og_mou_8', 'loc_og_mou_9', 'std_og_t2t_mou_6', 'std_og_t2t_mou_7',\n",
    "    'std_og_t2t_mou_8', 'std_og_t2t_mou_9', 'std_og_t2m_mou_6', 'std_og_t2m_mou_7', 'std_og_t2m_mou_8',\n",
    "    'std_og_t2m_mou_9', 'std_og_t2f_mou_6', 'std_og_t2f_mou_7', 'std_og_t2f_mou_8', 'std_og_t2f_mou_9',\n",
    "    'std_og_t2c_mou_6', 'std_og_t2c_mou_7', 'std_og_t2c_mou_8', 'std_og_t2c_mou_9', 'std_og_mou_6',\n",
    "    'std_og_mou_7', 'std_og_mou_8', 'std_og_mou_9', 'isd_og_mou_6', 'isd_og_mou_7', 'isd_og_mou_8',\n",
    "    'isd_og_mou_9', 'spl_og_mou_6', 'spl_og_mou_7', 'spl_og_mou_8', 'spl_og_mou_9', 'og_others_6',\n",
    "    'og_others_7', 'og_others_8', 'og_others_9', 'total_og_mou_6', 'total_og_mou_7', 'total_og_mou_8',\n",
    "    'total_og_mou_9', 'loc_ic_t2t_mou_6', 'loc_ic_t2t_mou_7', 'loc_ic_t2t_mou_8', 'loc_ic_t2t_mou_9',\n",
    "    'loc_ic_t2m_mou_6', 'loc_ic_t2m_mou_7', 'loc_ic_t2m_mou_8', 'loc_ic_t2m_mou_9', 'loc_ic_t2f_mou_6',\n",
    "    'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8', 'loc_ic_t2f_mou_9', 'loc_ic_mou_6', 'loc_ic_mou_7', 'loc_ic_mou_8',\n",
    "    'loc_ic_mou_9', 'std_ic_t2t_mou_6', 'std_ic_t2t_mou_7', 'std_ic_t2t_mou_8', 'std_ic_t2t_mou_9',\n",
    "    'std_ic_t2m_mou_6', 'std_ic_t2m_mou_7', 'std_ic_t2m_mou_8', 'std_ic_t2m_mou_9', 'std_ic_t2f_mou_6',\n",
    "    'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8', 'std_ic_t2f_mou_9', 'std_ic_t2o_mou_6', 'std_ic_t2o_mou_7',\n",
    "    'std_ic_t2o_mou_8', 'std_ic_t2o_mou_9', 'std_ic_mou_6', 'std_ic_mou_7', 'std_ic_mou_8', 'std_ic_mou_9',\n",
    "    'total_ic_mou_6', 'total_ic_mou_7', 'total_ic_mou_8', 'total_ic_mou_9', 'spl_ic_mou_6', 'spl_ic_mou_7',\n",
    "    'spl_ic_mou_8', 'spl_ic_mou_9', 'isd_ic_mou_6', 'isd_ic_mou_7']])\n",
    "df_noncat_std = pd.DataFrame(ss_fit, columns=df_noncat_imputed.columns+'_std'); df_noncat_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc173e83-2180-49aa-bddb-00dd0f9519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noncat_std=df_noncat_std.copy()\n",
    "df_dummy_cat=df_dummy_cat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c779de-8347-4f80-92c8-e4d1696da987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppd = pd.concat([df_noncat_std, df_dummy_cat], axis=1)\n",
    "df_ppd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e34bcb-df00-4bbe-8575-50488dff2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7d151-bb89-4231-ab9a-b0641a4bb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans=df_ppd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323bf8f-0bf1-4de0-b5f2-a5b47735b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb614c3-ce97-4906-b3e6-cdbc530494df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78bdaf-1a4f-4124-b90a-fa7858b02c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels=model.fit_predict(df_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c8e51-1b3c-4274-bf2e-3160fb1fa841",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae80cf4-ce56-45ad-8df6-7fbc40fe8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans['cluster']=cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d30ec-36f3-4688-86cb-f11157516710",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_kmeans.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c88098-1837-4951-a2e1-8b179bd3936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans.corr()['cluster'].iloc[:-1].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d65454-f70b-48e2-983c-226375a04e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "df_kmeans.corr()['cluster'].iloc[:-1].sort_values().plot(kind='bar')\n",
    "plt.xticks(rotation=90, ha='right', fontsize=5)  # Change the font size of x-axis labels to 12\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38e1a8-e601-49a0-8279-3965c9fd99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ssd = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(4, 10):\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(df_kmeans)\n",
    "    \n",
    "    # Sum of squared distances of samples to their closest cluster center\n",
    "    ssd.append(model.inertia_)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e657f8-3aee-41ec-ab9e-b341b9057aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cdbad-1407-4e04-9c9a-a39aa44ea7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(4,10),ssd,'o--')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Sum of Squared Distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a221e02-e8cf-448b-8274-2f1e705335c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372778c-d017-46ee-b86a-09c0978f3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_kmeans=df_ppd.sample(n=25000,random_state=822)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ab86e-f928-4ed5-87d0-3b13cfbe77d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ssd_sample = []\n",
    "silhouette_scores_sample = []\n",
    "\n",
    "for k in range(4, 10):\n",
    "    model_sample = KMeans(n_clusters=k, n_init=10)\n",
    "    model_sample.fit(sample_kmeans)\n",
    "    \n",
    "    # Sum of squared distances of samples to their closest cluster center\n",
    "    ssd_sample.append(model_sample.inertia_)\n",
    "    \n",
    "    # Silhouette score\n",
    "    silhouette_avg = silhouette_score(sample_kmeans, model_sample.labels_)\n",
    "    silhouette_scores_sample.append(silhouette_avg)\n",
    "\n",
    "# Print SSD and silhouette scores for each cluster\n",
    "for k, ssd_score, silhouette_score in zip(range(4, 10), ssd_sample, silhouette_scores_sample):\n",
    "    print(f\"Number of clusters: {k}, SSD: {ssd_score}, Silhouette Score: {silhouette_score}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a9a4bea-69d3-43b6-b5de-bae4ff169d75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b1209-25c6-4992-a7ab-64333db2c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heirarchical agglomerative Clustering\n",
    "import scipy.cluster.hierarchy as sch # For Hierarchical Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering as agclus, KMeans as kmclus # For Agglomerative & K-Means Clustering\n",
    "from sklearn.metrics import silhouette_score as sscore, davies_bouldin_score as dbscore # For Clustering Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715116f-2650-4331-b9df-bacef6c40ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_cluster=df_ppd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94459374-abaf-4f44-8c28-eec82730c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_agg_cluster=df_agg_cluster.sample(n=25000,random_state=822)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276eb20-4843-4120-bf00-fcfab21d0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_5cluster = agclus(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "ah_5cluster_model = ah_5cluster.fit_predict(sample_agg_cluster); ah_5cluster_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c9794-e5b9-4cab-8980-d840c1e5be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_6cluster = agclus(n_clusters=6, affinity='euclidean', linkage='ward')\n",
    "ah_6cluster_model = ah_6cluster.fit_predict(sample_agg_cluster); ah_6cluster_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d4f25-71ab-466b-b5a9-73ac8b9a2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_4cluster = agclus(n_clusters=4, affinity='euclidean', linkage='ward')\n",
    "ah_4cluster_model = ah_4cluster.fit_predict(sample_agg_cluster); ah_4cluster_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24547887-7176-4cc3-9bab-2f98735fbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_3cluster = agclus(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "ah_3cluster_model = ah_3cluster.fit_predict(sample_agg_cluster); ah_3cluster_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445376e-a484-4525-998b-294d245ebe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sscore_ah_3cluster = sscore(sample_agg_cluster, ah_3cluster_model); sscore_ah_3cluster -0.2883128555155335\n",
    "sscore_ah_4cluster = sscore(sample_agg_cluster, ah_4cluster_model); sscore_ah_4cluster- 0.28005227166850494\n",
    "sscore_ah_5cluster = sscore(sample_agg_cluster, ah_5cluster_model); sscore_ah_5cluster- 0.2777902860167923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab6770-01c3-420a-8461-397bbe6f8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbcore_ah_3cluster = dbscore(sample_agg_cluster, ah_3cluster_model); dbcore_ah_3cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164253e-7e68-4453-9ee5-311932cc6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sscore_ah_4cluster = sscore(sample_agg_cluster, ah_4cluster_model); sscore_ah_4cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14950633-c153-4f2a-bd22-b75a56f4a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbcore_ah_4cluster = dbscore(sample_agg_cluster, ah_4cluster_model); dbcore_ah_4cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4fd81a-96d0-4f51-84c6-e8012a2d2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sscore_ah_5cluster = sscore(sample_agg_cluster, ah_5cluster_model); sscore_ah_5cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16668384-98c6-4ceb-a080-e5e263e6e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscore_ah_5cluster = dbscore(sample_agg_cluster, ah_5cluster_model); dbscore_ah_5cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff2f38-1b3c-498e-9d31-5758d2b1b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "sscore_ah_6cluster = sscore(sample_agg_cluster, ah_6cluster_model); sscore_ah_6cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461d024-afea-46b6-a64a-8a211adf2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscore_ah_6cluster = dbscore(sample_agg_cluster, ah_6cluster_model); dbscore_ah_6cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6c7de-79af-4d21-b5b6-74d798d0a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ssd_sample = []\n",
    "silhouette_scores_sample = []\n",
    "\n",
    "for k in range(4, 10):\n",
    "    model_sample = KMeans(n_clusters=k, n_init=10)\n",
    "    model_sample.fit(sample_kmeans)\n",
    "    \n",
    "    # Sum of squared distances of samples to their closest cluster center\n",
    "    ssd_sample.append(model_sample.inertia_)\n",
    "    \n",
    "    # Silhouette score\n",
    "    silhouette_avg = silhouette_score(sample_kmeans, model_sample.labels_)\n",
    "    silhouette_scores_sample.append(silhouette_avg)\n",
    "\n",
    "# Print SSD and silhouette scores for each cluster\n",
    "for k, ssd_score, silhouette_score in zip(range(4, 10), ssd_sample, silhouette_scores_sample):\n",
    "    print(f\"Number of clusters: {k}, SSD: {ssd_score}, Silhouette Score: {silhouette_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788e61b-2d8c-492a-8a6e-7037690df4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924c20d-f7fb-45d4-8024-ac33b70accac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def measure_memory_usage():\n",
    "\n",
    "\n",
    "    ssd_sample = []\n",
    "    silhouette_scores_sample = []\n",
    "\n",
    "    for k in range(4, 10):\n",
    "        model_sample = KMeans(n_clusters=k, n_init=10)\n",
    "        model_sample.fit(sample_kmeans)\n",
    "        \n",
    "        # Sum of squared distances of samples to their closest cluster center\n",
    "        ssd_sample.append(model_sample.inertia_)\n",
    "        \n",
    "        # Silhouette score\n",
    "        silhouette_avg = silhouette_score(sample_kmeans, model_sample.labels_)\n",
    "        silhouette_scores_sample.append(silhouette_avg)\n",
    "\n",
    "        return\n",
    "%memit measure_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcf072-eb16-477d-b2c2-826499afa961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def measure_memory_usage():\n",
    "\n",
    "\n",
    "    ssd_sample = []\n",
    "    silhouette_scores_sample = []\n",
    "\n",
    "    for k in range(4, 10):\n",
    "        model_sample = KMeans(n_clusters=k, n_init=10)\n",
    "        model_sample.fit(sample_kmeans)\n",
    "        \n",
    "        # Sum of squared distances of samples to their closest cluster center\n",
    "        ssd_sample.append(model_sample.inertia_)\n",
    "        \n",
    "        # Silhouette score\n",
    "        silhouette_avg = silhouette_score(sample_kmeans, model_sample.labels_)\n",
    "        silhouette_scores_sample.append(silhouette_avg)\n",
    "\n",
    "        return\n",
    "%memit measure_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631dcb9-b80e-4793-98cc-bc88410c1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ah_5cluster = agclus(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "ah_5cluster_model = ah_5cluster.fit_predict(sample_agg_cluster); ah_5cluster_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd82f0c-ae10-44cb-9a90-753ce98c58ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def measure_memory_usage():\n",
    "\n",
    "    ah_5cluster = agclus(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "    ah_5cluster_model = ah_5cluster.fit_predict(sample_agg_cluster); ah_5cluster_model\n",
    "                        \n",
    "    return\n",
    "%memit measure_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddb9cb-f13f-490c-a148-1f26275ce37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
